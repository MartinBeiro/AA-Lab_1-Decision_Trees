{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9ff418",
   "metadata": {},
   "source": [
    "# Entrega 1 - Árboles de Decisión\n",
    "\n",
    "## Grupo 39\n",
    "- M. Beiro C.I.:\n",
    "- J. Rodríguez C.I.: 4.328.718-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa2070",
   "metadata": {},
   "source": [
    "## 1. Objetivo\n",
    "\n",
    "El objetivo de esta tarea es implementar un arbol de decisión utilizando el algoritmo de aprendizaje ID3.\n",
    "\n",
    "Se pretende escribir el código que genere un arbol de decisión para determinado set de datos y luego poder compararlo con los algoritmos *DecisionTreeClassifier* y *RandomForestClassifier* de la librería *scikit-learn*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae13a00",
   "metadata": {},
   "source": [
    "## 2. Diseño\n",
    "\n",
    "### 2.1 Preprocesamiento de los datos\n",
    "\n",
    "La base de datos usada es extraída del siguiente link:\n",
    "https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success\n",
    "\n",
    "En dicho link se encuentra la documentación en donde se puede apreciar que los datos pueden ser enteros, continuos o categóricos.\n",
    "En la descripción de cada atributo podemos ver que hay algunos cuyos valores enteros tienen una correspondencia con ciertas categorías, por lo que se entiende que indirectamente son categóricos a pesar de que su tipo es entero.\n",
    "\n",
    "Para los datos de tipo coninuo de deberá discretizar los valores.\n",
    "Antes de aplicar la discritización se visualiza la cantidad de valores diferentes por columna para ver cómo son los datos.\n",
    "A continuación se muestran las columnas con mas de 50 datos.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ceed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = 'data/data.csv'\n",
    "df_data = pd.read_csv(path, sep=';')\n",
    "\n",
    "df_cantidad_datos = df_data.nunique()\n",
    "df_cantidad_datos[df_cantidad_datos>50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89caf17",
   "metadata": {},
   "source": [
    "El resultado muestra que las columnas *Curricular units 1st sem (grade)* y *Curricular units 2nd sem (grade)* tienen más de 700 datos diferentes, mientras que la documentación indica que se trata de datos enteros entre 0 y 20. Observando los datos se puede ver que presentan valores decimales, por lo que para estas columnas el tratamiento que se realizará será de redondar las cifras al entero más cercano.\n",
    "\n",
    "En conclusión se realizarán dos tipo de procesmaientos:\n",
    "1. Redondeo para datos de tipo incorrecto según su documentación.\n",
    "2. Discretización para datos continuos.\n",
    "\n",
    "Para ello se implementa el código en el archivo *preprocesamiento.py*, el cual necesita de un archivo de configuración que indique a qué columna se le aplica un determinado procesamiento.\n",
    "A modo de ejemplo se expone en una tabla ese archivo.\n",
    "\n",
    "|            columna               | algoritmo| bins |\n",
    "|:---------------------------------|----------|------|\n",
    "| Previous qualification (grade)   | bins     | 20   |\n",
    "| Admission grade                  | bins     | 20   |\n",
    "| Curricular units 1st sem (grade) | redondeo | 0    |\n",
    "| Curricular units 2nd sem (grade) | redondeo | 0    |\n",
    "| Unemployment rate                | bins     | 10   |\n",
    "| Inflation rate                   | bins     | 10   |\n",
    "| GDP                              | bins     | 10   |\n",
    "\n",
    "\n",
    "Para la discretización por bines se utiliza  la función *KBinsDiscretizer* de la librería *scikit-learn*, a la cual se le indica la cantidad de bines de acuerdo al archivo de configuración.\n",
    "\n",
    "Adicional a eso se reemplaza los valores 'Enrolled' de la columna 'Target' por el valor 'Graduate' de acuerdo a la letra del problema.\n",
    "\n",
    "La separación de datos de entrenamiento y testeo se realiza con la función *train_test_split* de la librería *scikit-learn*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f9dba",
   "metadata": {},
   "source": [
    "### 2.2 Algoritmo\n",
    "\n",
    "Para la construcción del arbol de decisión se implementa el algorítmo ID3.\n",
    "\n",
    "Se parte de la base de un set de datos que contiene la clasificación deseada, la cual llamaremos 'target'.\n",
    "Dicho algortimo comienza questionando qué atributo clasifica mejor a los datos.\n",
    "Para responder esa pregunta se utiliza la medida de la ganancia de entropía que tiene la siguiente forma:\n",
    "\n",
    "$$Ganancia (S,A)=Entropia(S) - \\sum_{v\\in Val(A)} \\frac{|S_v|}{|S|}Entropia(S_v) $$\n",
    "\n",
    "Donde la entropía de un conjunto se calcula como:\n",
    "\n",
    "$$Entropia(S)=-\\sum_{c_i\\in C}p_{c_i}.log(p_{c_i})$$\n",
    "\n",
    "Donde cada $p_{c_i}$ es la proporción cada valor del atributo target.\n",
    "\n",
    "Luego se separa el set de datos filtrando por cada valor único del atributo y para cada sub set se realiza el mismo procedimiento recursivamente.\n",
    "\n",
    "Se utiliza como método de parada dos hiperparámetros llamados *min_split_gain* y *min_samples_split*. El primero determina la gantidad mínima de ganancia que debe aportar un atributo para continuar con la recursividad. En caso que no la cumpla simplemente se crea una hoja con el valor más común del sub set de datos. El segundo hiperparámentro indica cula es la cantidad mínima de datos para seguir adelante con la recursividad. Si el subset de datos no llega a esa cantidad, se crea una hoja con el calor más común.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5147cc2f",
   "metadata": {},
   "source": [
    "### 2.3 Evaluación\n",
    "\n",
    "Para la evaluación del algoritmo se separa el set de datos en train y test. Se construirá el arbol de decisión utilizando el método explicado anteriormente sobre el set de entrenamiento.\n",
    "Luego se utilizará el set de testeo para evaluar el desempeño en función de los aciertos que obtenga cada uno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be982912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import main\n",
    "\n",
    "df_data=main.cargar_datos()\n",
    "df_preprod_config = main.cargar_configuracion()\n",
    "main.preprocesar_datos(df_data, df_preprod_config)\n",
    "df_train, df_test = main.separar_train_test(df_data, test_size=0.3, random_state=45)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a99338",
   "metadata": {},
   "source": [
    "Para evaluar como varían los resultados en función de los hiperparámetros se realiza el método 'grid serach' y se grafican los resultados.\n",
    "Se pretende vizualizar el impacto de los mencionados hiperparámetros tanto en la exactitud de la predicción como en la cantidad de hojas del arbol generado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4981359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "min_samples_split_array = np.arange(40, 200,100)\n",
    "min_split_gain_array = np.arange(0.03, 0.3, 0.1)\n",
    "\n",
    "main.evaluar_hiperparametros(df_train, df_test, min_samples_split_array, min_split_gain_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff314919",
   "metadata": {},
   "source": [
    "Se observa como a medida que aumentamos el *min_samples_split* se perjudica la exactitud. Sucede lo mismo al aumentar el *min_split_gain*. Este resultado es el esperado ya que al tener condiciones de parada más restrictivas, se obtiene un arbol menos preciso. Por otro lado se observa como la cantidad de hojas de los árboles disminuye a mendida que aumentamos los hiperparámetros. Árboles más grandes generan resultados más exactos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b503a51d",
   "metadata": {},
   "source": [
    "A continuación se elige un valor para cada hiperparámetro y se compara con los algoritmos DecisionTreeClassifier y RandomForestClassifier de la librería scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d9ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_split = 40\n",
    "min_split_gain = 0.2\n",
    "\n",
    "main.arbol_ID3(df_train, df_test, min_samples_split, min_split_gain)\n",
    "\n",
    "main.arbol_DTC(df_train, df_test, min_samples_split)\n",
    "\n",
    "main.arbol_RTF(df_train, df_test, min_samples_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca3799b",
   "metadata": {},
   "source": [
    "Los resultados se muestran en la siguiente tabla:\n",
    "    \n",
    "|           | ID3 | DTC  | RFC  |\n",
    "|----------:|-----|------|------|\n",
    "| Exactitud | --  | 0.86 | 0.87 |\n",
    "\n",
    "Para el caso de los hiperparámetros seleccionados, el algoritmo *Random Forest Classifier* resultó ser el más exacto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645c63d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
